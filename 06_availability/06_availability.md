# Высокая доступность кластера

Поговорим о таком свойстве системы как доступность.
**Доступность (availability)** - любой запрос может быть обработан системой, вне зависимости от ее состояния.

![image](https://user-images.githubusercontent.com/32142520/114453293-101a3800-9be2-11eb-8e6d-76e654dc4595.png)

Построение распределенной системы позволяет решить многие проблемы, которые возникают при разработке высоконагруженных приложений.
Но вместе с тем распределенные системы значительно сложнее приложений, которые работают в рамках одного сервера.
Эта сложность порождает невообразимое множество проблем, которые могут привести систему в нерабочее состояние самым неожиданным образом.

Логично предположить, что высоконагруженные приложения должны предоставлять пользователям гарантии.
Система, которая находится в нерабочем состоянии и теряет данные пользователей довольно быстро не выдержит конкуренции и перестанет быть высоконагруженной.

Одно из достоинств распределенных приложений это то что потенциально при отказе части системы она может продолжать работать так, чтобы конечный пользователь не
почувствовал разницы.
Но добиться этого не так-то просто.

В реальной жизни распределенное приложение почти никогда не пребывает в состоянии, когда все узлы работают нормально.
Поэтому при проектировании нужно продумать, как заставить систему работать таким образом, чтобы она выполняла свое предназначение и гарантировала конечному пользователю целостность его данных.

## Создание надежной системы из ненадежных компонентов

На первый взгляд кажется, что система не может быть надежнее, чем самый ненадежный ее компонент.
Но на деле практика построение надежной системы из ненадежных компонентов широко распространена в IT.
Например, TCP-протокол увеличивает надежность отправки пакетов через ненадежный IP, но и он не может устранить сетевые задержки.
Использование самокорректирующихся кодов (вроде кода Хемминга) позволяет исправлять ошибки в нескольких битах, но не поможет при потере большей части слова.

Такой подход устраняет некоторые сбои, делая систему более стабильной и упрощая обработку ошибок на других уровнях.

## Возможные причины сбоев

![image](https://user-images.githubusercontent.com/32142520/114438066-f243d780-9bcf-11eb-9227-0fbc0e4a6a37.png)

Для того чтобы представить себе, какие проблемы необходимо предусмотреть, нужно максимально пессимистично взглянуть на все части системы и мысленно разломать все что только можно.

**Если что-то может пойти не так, оно пойдёт не так.**

### Сетевые проблемы

В распределенной системе связь между узлами осуществляется по сети.
Обмен данными происходит исключительно по сети, узлы не имеют доступа к данным других узлов.

Существует два подхода передачи данных по сети:

* асинхронный - данные считаются переданными без подтверждения со стороны адресата;
* синхронный - отправитель ожидает подтверждения.

Хотя синхронный подход выглядит более надежным, поскольку обеспечивает передачу всех пакетов в правильном порядке, он совершенно не помогает в локализации проблем с сетью и не способствует их решению.
Отсутствие ответа может означать что угодно:

* запрос был потерян;
* запрос попал в очередь и будет отправлен позже;
* запрос был доставлен, но на принимающей стороне произошла необработанная ошибка;
* запрос был доставлен, но ответ не дошел до отправителя или попал в очередь и будет доставлен позже.

Распространенная практика решения таких проблем - установка времени ожидания, после которого запрос считается не доставленным.
В таком случае, система должна корректно обрабатывать повторные запросы, чтобы не прийти в нерабочее состояние.

ЦОДы в основном используют асинхронные протоколы.
Узел отправляет запрос, но сеть не дает гарантий, что запрос будет доставлен быстро и будет ли доставлен вообще.

Сетевые сбои происходят регулярно и по самым разным причинам:

* При перестройке топологии сети могут возникать длительные задержки.
* Сетевой интерфейс может совершенно неожиданно перестать обрабатывать входящие запросы, но продолжать отправлять исходящие. Такую проблему очень сложно диагностировать.
* Физические повреждения составных частей сети - акула может прокусить подводный кабель, ЦОД может быть затоплен или обесточен, в него может врезаться автомобиль.

Ситуация, когда часть сети становится отрезанной от остальной сети называется
**нарушением связности сети (network partition, netsplit)**.

Система должна предполагать, что сетевые сбои будут случаться неизбежно и должны быть корректно обработаны.
Они не должны приводить к блокировке узлов, все ошибки должны быть корректно обработаны, пользователь должен получить адекватное сообщение об ошибке.

Хорошей практикой является тестирование системы на устойчивость к различным сетевым сбоям и проработка сценариев восстановления после таких сбоев.

## Обнаружение сбоев

Система должна обнаруживать неисправные узлы и перестраиваться таким образом, чтобы наладить работу.
Балансировщик нагрузки должен перестать отправлять запросы на узлы, которые долго не отвечают.
При использовании репликации после выведения из строя мастера одна из реплик должна занять его место.

Дополнительные сложности вносят проблемы с сетью - не всегда понятно, вышел ли узел из строя или с ним просто потеряна связь.

Возможные подходы к определению нерабочих узлов:

* Если при запросе на машину ни один процесс не прослушивает запрошенный порт, операционная система будет закрывать сетевые соединения или отказывать их устанавливать.
* При сбое узла операционная система может собрать данные о сбое и оповестить остальные узлы, что нужно перераспределить задачи.
* Доступ к коммутаторам сети через административный интерфейс позволяет установить проблемы на аппаратном уровне.
* Маршрутизатор может определить недоступность IP-адреса узла и вернуть ошибку Destination Unreachable.

В случае проблем с подключением к удаленному узлу может быть получена ошибка на одном из уровней стека.
При этом есть смысл попытаться отправить запрос и дождаться ответа еще несколько раз.

### А сколько ждать?

Выбор адекватного времени ожидания непрост.
При большом времени ожидания приложение может долгое время находиться в нерабочем состоянии, пользователи будут получать ошибки.
Слишком маленькое время ожидания может привести к объявлению рабочего узла нерабочим.
Если узел при этом выполняет какую-либо задачу, то в случае признания узла неисправным, мы рискуем продублировать эту задачу.
В некоторых случаях такая ситуация крайне нежелательна - вряд ли пользователи будут рады продублированному списанию средств с карты.

Если узел отвечает медленно из-за сильной нагрузки, то выведение этого узла из эксплуатации только усугубит ситуацию.

В случае, если время отправки запроса ограничено сверху (пакет доставляется или теряется в течение фиксированного времени) и обработка запроса занимает некоторое известное время, можно подобрать оптимальное время ожидания.

К сожалению, в реальности все не так прозрачно.
Асинхронные сети не имеют ограничений по задержкам, а обработка запросов за определенное время почти никогда не гарантирована.
Для того чтобы обнаружить проблемы, необходимо чтобы система работала исправно большую часть времени.
Тогда скачки времени на отправку и обработку запросов будут заметны при небольшом времени ожидания.

### Что влияет на время отклика?

* При большом количестве отправляемых пакетов может образоваться очередь.
  Как следствие, сеть перегружена, и время отклика увеличено.
  Если очередь заполнена пакетами, то новые пакеты удаляются и должны быть отправлены заново.
* Большое количество принимаемых пакетов ведет к загрузке CPU и входящие запросы тоже отправляются в очередь.
* Виртуальные машины могут блокироваться, пока CPU используется другой машиной.
* TCP может ограничивать число отправляемых сообщений, из-за чего запросы попадают в очередь.

## Chaos Engineering

**Хаос-инжиниринг — это подход, предусматривающий проведение экспериментов над production-системой, чтобы убедиться в ее способности выдерживать различные помехи, возникающие во время работы.**

Компания Netflix, занимающаяся онлайн-распространением видео-контента, придерживается довольно жестких принципов для поддержания доступности системы.
Доступность измеряется как отношение успешных попыток запустить фильм к общему числу попыток.
Целевое значение - 0,9999.
И довольно часто оно достигается.

Компания использует несколько приложений, которые провоцируют проблемы в системе и позволяют выявить проблемы на раннем этапе [1].

* Chaos Monkey - выбирает случайный инстанс на продакшене и убивает его.
* Chaos Gorilla - отключает одну из зон доступности в AWS.
* Chaos Kong - отключает регион AWS.

![image](https://user-images.githubusercontent.com/32142520/114440238-90d13800-9bd2-11eb-90be-2cf52e2e438b.png)

## Отказ реплики: Recovery

При отказе реплики (или после разрыва соединения с мастером) данные могут быть восстановлены следующим образом.
После рестарта реплики из лога могут быть определены последние изменения, которые произошли перед падением.
Далее, реплика подключается к лидеру и запрашивает изменения, которые произошли после отказа.
После того как реплика "догнала" мастера, она может продолжить принимать данные в обычном режиме.

## Отказ лидера: Failover

Если в приложении с master-slave репликацией отказал лидер, то должно произойти **аварийное переключение (failover)**:

* одна из реплик становится новым лидером;
* запросы на запись должны быть перенаправлены на нового лидера;
* реплики должны начать реплицировать данные с нового лидера.

Failover может быть произведен вручную или автоматически.
Нас интересует автоматический процесс.

1. Первое, что нужно сделать - **обнаружить падение лидера**.
   Не существует достоверного способа установить падение узла, поэтому большинство систем используют таймауты.
   Если узел не отвечает в течение выделенного таймаута, он считается неисправным.

2. Далее происходит **выбор нового лидера**.
   Он может быть выбран в соответствии с приоритетностью оставшихся реплик
   или назначен заранее определенным координатором.
   Наилучшим кандидатом, как правило, является реплика с наиболее актуальными данными.

3. **Перестройка системы** в соответствии с новым лидером.
   Запросы должны быть перенаправлены на нового лидера.
   Если старый лидер вернется в строй, он должен стать репликой и подключиться с новому лидеру.

### Failover: Что может пойти не так?

* При асинхронной репликации *новый лидер может не успеть получить последние изменения от старого*.
  Что делать с этими изменениями после возвращения старого лидера?
  Чаще всего они сбрасываются, поскольку новый лидер мог успеть обработать конфликтующие запросы на запись.

* Дополнительную опасность в таких случаях представляет использование внешних систем, которые координируются теми же данными.
  В 2012 году произошел инцидент на GitHub.
  "Отстающая" реплика была назначена новым лидером, но при этом последние значения autoincremented primary key были переиспользованы.
  Эти ключи использовались внешним хранилищем для идентификации данных пользователей, что привело к раскрытию приватных данных между юзерами.

* Может случиться так, что два узла считают себя лидерами - **split-brain**.
  В таком случае если оба узла получат запросы на запись, то может случиться конфликт или потеря данных.
  Некоторые системы в таких случаях отключают один из узлов, но если этот механизм работает неточно, то можно потерять оба узла.

Простого решения у этих проблем нет.
Часто failover требует ручного вмешательства даже при наличии автоматики.

## Предотвращение split-brain

Одна из необходимых абстракций, которые помогают приложению обрабатывать различные сбои - это **консенсус**.
Идея очень проста - *все узлы системы должны быть согласны в чем-то*.

Чтобы исключить ситуацию, когда несколько узлов одновременно считают себя лидерами, используется lock.
Каждая нода пытается захватить lock, и первый узел, который сможет это сделать, становится лидером.
Детали реализации не имеют значения, но этот lock должен обладать следующим свойством - все узлы должны четко понимать, кто завладел данным lock'ом.

В качестве таких lock'ов часто используются такие сервисы, как etcd, Apache ZooKeeper и Consul.
Их часто называют "распределенными key-value хранилищами" или “сервисами конфигурации и координации".
Такие сервисы используют алгоритмы консенсуса, которые позволяют реализовать отказоустойчивые **линеаризуемые** операции со значениями.
Основная идея таких операций - все действия происходят как будто бы атомарно и существует как будто бы одна единственная копия данных.

Использование внешнего координатора - необходимое, но не достаточное условие для реализации правильного переключения лидера.

## Fencing

При использовании lock'ов существует ряд проблем.

Допустим, у нас есть сторадж, в который может одновременно писать один клиент, чтобы не была нарушена целостность данных.
Если попытаться реализовать это с помощью внешнего lock-сервиса, то возможно следующая ситуация:

![image](https://user-images.githubusercontent.com/32142520/114463817-9f791880-9bed-11eb-8b6c-c16840846203.png)

* первый клиент берет lock на запись в файл и "подвисает";
* за время подвисания lock освобождается, его захватывает второй клиент и выполняет запись в файл;
* первый клиент возвращается и в полной уверенности, что lock принадлежит ему, тоже пишет в файл.

Чтобы предотвратить подобные ситуации, мы должны убедиться, что узел не просто верит в то, что он "избранный", а он действительно такой.
Этого легко достичь при использовании такой техники как **fencing**.

![image](https://user-images.githubusercontent.com/32142520/114464828-f4695e80-9bee-11eb-85aa-5d2b920567b9.png)

Каждый успешный lock возвращает монотонно возрастающий *fencing token*.
Каждая запись в сторадж требует от клиента актуальный токен.

* первый клиент берет lock на запись в файл, получает токен *33* и "подвисает";
* за время подвисания lock освобождается, его захватывает второй клиент уже с токеном *34* и выполняет запись в файл, передав стораджу токен *34*;
* первый клиент возвращается и в полной уверенности, что lock принадлежит ему, тоже пишет в файл с устаревшим токеном *33*.
  Этот запрос отклоняется - уже был получаен запрос с более новым токеном.

Обратите внимание, что проверки токенов на стороне клиентов недостаточно, нужна проверка со стороны сервера.

Для построения корректного алгоритма фенсинга, нужно соблюсти следующие свойства:

* уникальность - для каждого запроса возвращается уникальный токен;
* монотонность - токен запроса, произошедшего позже, всегда больше токена запроса, который произошел раньше;
* доступность - если узел запросил токен, то он его получит, если неожиданно не прекратит работу.

Алгоритм является корректным, если он удовлетворяет всем этим свойствам в любой момент времени.
Но в случае если все узлы вышли из строя или задержки сети стали бесконечно долгими, ни один алгоритм не будет работать.

## Безопасность и живучесть

Уникальность и монотонность - свойства безопасности.
Доступность - свойство живучести.

Безопасность - **ничего плохого не произойдет**.
Если свойство безопасности было нарушено, то мы можем определить момент времени, когда это произошло, и начиная с этого момента нанесенный урон не может быть устранен.

Живучесть - **рано или поздно произойдет что-то хорошее** (eventually).
Свойство может не выполняться в конкретный момент времени, но через какое-то время оно может быть достигнуто.

## Внешние координаторы - etcd, ZooKeeper, Consul

API подобных сервисов, как правило, позволяет читать и записывать значения по ключу и итерироваться по представленным ключам.
Но все же это не база данных.

Внешние хранилища позволяют хранить небольшое количество данных в памяти (конечно, эти данные также записываются на диск).
Эти данные распространяются среди подключенных узлов при помощи отказоустойчивых алгоритмов.
Получается что-то похожее на репликацию - всякое изменение данных применяется ко всем узлам.

Внешние координаторы часто используются для **service discovery**, когда требуется получить IP-адрес сервиса.
Например, при старте виртуальной машины, она регистрирует свой IP-адрес в **service registry**.
Обычно для этих целей используется DNS, где используется кеширование и прочие оптимизации.
Небольшое "устаревание" данных из DNS обычно не является проблемой, важнее то, что DNS устойчив к перебоям в сети и отличается высокой доступностью.

**ZooKeeper**:

* инструмент для service discovery и хранения конфигурации;
* изначально использовался в Hadoop кластерах, наиболее зрелое решение;
* высокая производительность
* поддержка Kafka;

**etcd**:

* надежное key-value хранилище;
* open-source;
* новое решение, которое отличается простотой и удобством использования;

**Consul**:

* сервис для хранения конфигурации, распределенной синхронизации;
* наиболее широкое решение, включает в себя встроенный service discovery;
* высоко доступный service discovery;
* health checking;
* gossip протоколы для кластера;
* интеграция с Docker.

## Практика

Реализуем распределенный lock для выбора лидера при помощи etcd.

## Ссылки

[1]: https://netflixtechblog.com/chaos-engineering-upgraded-878d341f15fa

## Домашнее задание

Реализовать аварийное переключение лидеров при помощи etcd.
Написать тест, который проверяет, что после отказа лидера кластер продолжает функционировать и конфигурация VShard одинакова на всех инстансах.

(*) Для получения доп. баллов предлагается написать стресс-тесты - пусть приложение принимает данные в течение некоторого времени. Во время переключения лидера часть данных будет потеряна. Нужно померить количество потерянных данных.

### Детали

* Поднимаем приложение из примера - там один роутер и 2 сторадж-репликасета. Для чистоты эксперимента лучше добавить еще по одному стораджу, чтобы было по 2 реплики в каждом. Так получится протестировать, что выбирается одна из двух доступных реплик при отключении мастера.

* Добавляем простое API (можно HTTP, можно по iproto) - put и get. (API предоставляет роутер, для каждого запроса он выбирает нужный сторадж и выполняет на нем insert/get).

* Убиваем мастера одного из репликасетов - put запросы перестают работать, get запросы работают.

* Через некоторое время срабатывает наш failover - put запросы начинают работать.

* В качестве основы можно взять [example](https://github.com/tarantool/vshard/tree/master/example) из репозитория vshard.

* Тесты могут быть любыми. Единственное условие - в `README.md` должна быть внятная инструкция по их запуску (желательно одной командой). Можно использовать Makefile, bash-скрипт, pytest, что угодно.

### Подсказки

* [Документация](https://www.tarantool.io/ru/doc/latest/reference/reference_rock/vshard/vshard_api/) может ответить на многие вопросы.

* Для записи/чтения нужно использовать `vshard.router.call{rw,ro}(bucket_id, func_name, func_args)`.

* `func_name` - функция на сторадже, которая выполняет insert/get на сторадже. Чтобы эту функцию можно было вызывать с роутера, ее нужно положить в `_G.<func_name>`.

* `bucket_id` считаем от primary key (в целом можно считать как угодно, лишь бы всегда одинаково).
