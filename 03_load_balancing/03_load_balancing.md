### Балансировка нагрузки
Вопрос балансировки нагрузки остро встает, когда мощностей одного сервера
перестает хватать для обслуживания входящих запросов.
В таком случае есть два варианта:
  - Вертикальное масштабирование — увеличение мощности одного сервера.
  Это может достигаться покупкой более мощного сервера или апгрейдом уже
  существующего. В любом случае это достаточно затратный путь — невозможно бесконечно
  масштабировать один сервер. Кроме того, сервер представляет из себя единую точку отказа —
  запросы перестают обрабатываться, если такой сервер по какой-то причине выйдет из строя.
  - Горизонтальное масштабирование — наращивание мощности путем увеличения количества серверов
  и распределения нагрузки между ними. В данном случае мы можем не вкладывать деньги в
  один большой сервер, а просто покупать несколько «средних» по мощности серверов.
  При этом если из строя выйдет один из серверов, то запросы всё равно продолжат обрабатываться,
  просто увеличится нагрузка на другие сервера.

![Image](https://miro.medium.com/max/4800/1*tEaZGz-p1-E2ytNjl5RPJg.jpeg)


Балансировка нагрузки бывает двух типов:
 - Аппаратная
 - Программная
 
 Аппаратная балансировка нагрузки производится специальным устройством,
 специально предназначенным для распределения сетевого трафика.
 Так же такие устройства обычно заточены для работы с шифрованным трафиком,
 и это является довольно привлекательной возможностью - аппаратное
 шифрование обычно быстрее программного.
 Однако в нашем курсе мы не будем останавливаться на аппаратной
 балансировке нагрузки.
 
 Программная балансировка нагрузки производится при помощи
 специального программного обеспечения. Этот подход
 является в чем-то более гибким — требуется просто зарезервировать
 отдельный сервер, который будет распределять входящие запросы по другим серверам.
 Перечень подобного программного обеспечения в настоящее время довольно широк [1].
 Поэтому при выборе решения стоит сразу учитывать специфику своей задачи
 и выбирать [2]:
 - Уровень, на котором будет происходить распределение входящего трафика.
 - Алгоритм балансировки

 ### Три уровня балансировки
 
![image](https://user-images.githubusercontent.com/8830475/111399035-97ed4f00-86d5-11eb-9efd-3c60d72c8e6a.png)

 Процедура балансировки осуществляется при помощи целого комплекса алгоритмов и методов, соответствующим следующим уровням модели OSI:
  - сетевому;
  - транспортному;
  - прикладному.

#### Балансировка на сетевом уровне
Балансировка на сетевом уровне предполагает решение следующей задачи: нужно сделать так, чтобы за один конкретный IP-адрес сервера отвечали разные физические машины. Такая балансировка может осуществляться с помощью множества разнообразных способов.

* DNS-балансировка. На одно доменное имя выделяется несколько IP-адресов.
Сервер, на который будет направлен клиентский запрос, обычно определяется с помощью алгоритма Round Robin
(о методах и алгоритмах балансировки будет подробно рассказано ниже).
* Балансировка по IP с использованием дополнительного маршрутизатора.
* Балансировка по территориальному признаку осуществляется путём размещения одинаковых
сервисов с одинаковыми адресами в территориально различных регионах Интернета.

#### Балансировка на транспортном уровне (L4)
Этот вид балансировки является самым простым: клиент обращается к балансировщику,
тот перенаправляет запрос одному из серверов, который и будет его обрабатывать.
Выбор сервера, на котором будет обрабатываться запрос, может осуществляться в соответствии с самыми разными алгоритмами
(об этом ещё пойдёт речь ниже): путём простого кругового перебора, путём выбора наименее загруженного сервера из пула и т.п.
Иногда балансировку на транспортном уровне сложно отличить от балансировки на сетевом уровне.

Различие между уровнями балансировки можно объяснить следующим образом.
К сетевому уровню относятся решения, которые не терминируют на себе пользовательские сессии.
Они просто перенаправляют трафик и не работают в проксирующем режиме.
На сетевом уровне балансировщик просто решает, на какой сервер передавать пакеты.
Сессию с клиентом осуществляет сервер.

На транспортном уровне общение с клиентом замыкается на балансировщике, который работает как прокси.
Он взаимодействует с серверами от своего имени, передавая информацию о клиенте в дополнительных данных и заголовках.
Таким образом работает, например, популярный программный балансировщик HAProxy.

#### Балансировка на прикладном уровне (L7)
При работе на прикладном уровне балансировщик работает в режиме «умного прокси».
Он анализирует клиентские запросы и перенаправляет их на разные серверы в зависимости от характера запрашиваемого контента. 
Так работает, например, веб-сервер Nginx, распределяя запросы между фронтендом и бэкендом.
За балансировку в Nginx отвечает модуль Upstream.
 
### Алгоритмы балансировки нагрузки
Существует много различных алгоритмов и методов балансировки нагрузки. Выбирая конкретный алгоритм, нужно исходить, во-первых, из специфики конкретного проекта, а во-вторых — из целей. которые мы планируем достичь.

В числе целей, для достижения которых используется балансировка, нужно выделить следующие:

- справедливость: нужно гарантировать, чтобы на обработку каждого запроса выделялись системные ресурсы и не допустить возникновения ситуаций, когда один запрос обрабатывается, а все остальные ждут своей очереди;
- эффективность: все серверы, которые обрабатывают запросы, должны быть заняты на 100%; желательно не допускать ситуации, когда один из серверов простаивает в ожидании запросов на обработку (сразу же оговоримся, что в реальной практике эта цель достигается далеко не всегда);
- сокращение времени выполнения запроса: нужно обеспечить минимальное время между началом обработки запроса (или его постановкой в очередь на обработку) и его завершения;
- сокращение времени отклика: нужно минимизировать время ответа на запрос пользователя.

Очень желательно также, чтобы алгоритм балансировки обладал следующими свойствами:

- предсказуемость: нужно чётко понимать, в каких ситуациях и при каких нагрузках алгоритм будет эффективным для решения поставленных задач;
- равномерная загрузка ресурсов системы;
- масштабирумость: алгоритм должен сохранять работоспособность при увеличении нагрузки.

#### Round Robin

![image](https://user-images.githubusercontent.com/8830475/112002930-7525c500-8b31-11eb-9c4a-e2a83aeff44f.png)

Round Robin, или алгоритм кругового обслуживания, представляет собой перебор по круговому циклу: первый запрос передаётся одному серверу, затем следующий запрос передаётся другому и так до достижения последнего сервера, а затем всё начинается сначала.
Алгоритм достаточно прост, но имеет целый ряд существенных недостатков.
Чтобы распределение нагрузки по этому алгоритму отвечало упомянутым выше критериями справедливости и эффективности, нужно, чтобы у каждого сервера был в наличии одинаковый набор ресурсов.
При выполнении всех операций также должно быть задействовано одинаковое количество ресурсов.
В реальной практике эти условия в большинстве случаев оказываются невыполнимыми.

Также при балансировке по алгоритму Round Robin совершенно не учитывается загруженность того или иного сервера в составе кластера.
Представим себе следующую гипотетическую ситуацию: один из узлов загружен на 100%, в то время как другие — всего на 10 - 15%. 
Алгоритм Round Robin возможности возникновения такой ситуации не учитывает в принципе, поэтому перегруженный узел все равно будет получать запросы.
Ни о какой справедливости, эффективности и предсказуемости в таком случае не может быть и речи.
В силу описанных выше обстоятельств сфера применения алгоритма Round Robin весьма ограничена.

#### Weighted Round Robin

![image](https://user-images.githubusercontent.com/8830475/112003029-91296680-8b31-11eb-85cb-b52b59f7aacc.png)

Это — усовершенствованная версия алгоритма Round Robin.
Суть усовершенствований заключается в следующем: каждому серверу присваивается весовой коэффициент в соответствии с его производительностью и мощностью.
Это помогает распределять нагрузку более гибко: серверы с большим весом обрабатывают больше запросов.
Однако всех проблем с отказоустойчивостью это отнюдь не решает.
Более эффективную балансировку обеспечивают другие методы,
в которых при планировании и распределении нагрузки учитывается большее количество параметров.

#### Least Connections

![image](https://user-images.githubusercontent.com/8830475/112003340-dfd70080-8b31-11eb-80a7-04c0f5ca6a06.png)

В предыдущем разделе мы перечислили основные недостатки алгоритма Round Robin.
Назовём ещё один: в нём совершенно не учитывается количество активных на данный момент подключений.

Рассмотрим практический пример. Имеется два сервера — обозначим их условно как А и Б.
К серверу А подключено меньше пользователей, чем к серверу Б.
При этом сервер А оказывается более перегруженным.
Как это возможно? Ответ достаточно прост: подключения к серверу А поддерживаются в течение более долгого времени по сравнению с подключениями к серверу Б.

Описанную проблему можно решить с помощью алгоритма, известного под названием least connections (сокращённо — leastconn).
Он учитывает количество подключений, поддерживаемых серверами в текущий момент времени.
Каждый следующий вопрос передаётся серверу с наименьшим количеством активных подключений.

Существует усовершенствованный вариант этого алгоритма, предназначенный в первую очередь для использования в кластерах,
состоящих из серверов с разными техническими характеристиками и разной производительностью.
Он называется Weighted Least Connections и учитывает при распределении нагрузки не только количество активных подключений,
но и весовой коэффициент серверов.

В числе других усовершенствованных вариантов алгоритма Least Connections следует прежде всего выделить Locality-Based Least Connection Scheduling и Locality-Based Least Connection Scheduling with Replication Scheduling.

Первый метод был создан специально для кэширующих прокси-серверов. 
Его суть заключается в следующем: наибольшее количество запросов передаётся серверам с наименьшим количеством активных подключений.
За каждым из клиентских серверов закрепляется группа клиентских IP.
Запросы с этих IP направляются на «родной» сервер, если он не загружен полностью.
В противном случае запрос будет перенаправлен на другой сервер (он должен быть загружен менее чем наполовину).

В алгоритме Locality-Based Least Connection Scheduling with Replication Scheduling каждый IP-адрес или группа IP-адресов закрепляется не за отдельным сервером,
а за целой группой серверов.
Запрос передаётся наименее загруженному серверу из группы.
Если же все серверы из «родной» группы перегружены, то будет зарезервирован новый сервер.
Этот новый сервер будет добавлен к группе, обслуживающей IP, с которого был отправлен запрос.
В свою очередь наиболее загруженный сервер из этой группы будет удалён — это позволяет избежать избыточной репликации.

#### Destination Hash Scheduling и Source Hash Scheduling
Алгоритм Destination Hash Scheduling был создан для работы с кластером кэширующих прокси-серверов,
но он часто используется и в других случаях.
В этом алгоритме сервер, обрабатывающий запрос, выбирается из статической таблицы по IP-адресу получателя.

Алгоритм Source Hash Scheduling основывается на тех же самых принципах, что и предыдущий, только сервер,
который будет обрабатывать запрос, выбирается из таблицы по IP-адресу отправителя.

#### Sticky Sessions

![image](https://user-images.githubusercontent.com/8830475/112003752-3e03e380-8b32-11eb-8158-1c60a67023bf.png)

Sticky Sessions — алгоритм распределения входящих запросов, при котором соединения передаются на один и тот же сервер группы. Он используется, например, в веб-сервере Nginx.
Сессии пользователя могут быть закреплены за конкретным сервером с помощью метода IP hash.
С помощью этого метода запросы распределяются по серверам на основе IP-aдреса клиента. Как указано в документации, «метод гарантирует, что запросы одного и того же клиента будет передаваться на один и тот же сервер».
Если закреплённый за конкретным адресом сервер недоступен, запрос будет перенаправлен на другой сервер.

Применение этого метода сопряжено с некоторыми проблемами.
Проблемы с привязкой сессий могут возникнуть, если клиент использует динамический IP.
В ситуации, когда большое количество запросов проходит через один прокси-сервер, балансировку вряд ли можно назвать эффективной и справедливой.
Описанные проблемы, однако, можно решить, используя cookies.

### Пример
Начнем с простого примера. Он довольно далек от реальной жизни,
но демонстрирует проблему, возникающую перед нами.
Создадим простой http-сервер.

```lua
#!/usr/bin/env tarantool

local digest = require('digest')
local http_server = require('http.server')

local function handler()
    local str = string.format("I'm %s", 8080)
    for _ = 1, 1e2 do
        digest.sha512_hex(str)
    end

    return {
        body = str,
        status = 200,
    }
end

local httpd = http_server.new('0.0.0.0', '8080', {log_requests = false})
httpd:route({method = 'GET', path = '/'}, handler)
httpd:start()
```

Сервер просто считает 100 раз хэш от строки.
При этом хэш криптографический, а значит вычислительно трудоемкий.
Для того, чтобы продемонстрировать, сколько запросов может такой сервер выполнить
мы воспользуемся специальной программой - wrk. Это инструмент нагрузочного тестирования.

```lua
-- load.lua
function request()
    return wrk.format('GET', '/')
end
```

Запустим сервер `tarantool -i single.lua`, а затем начнем делать запросы -
`wrk -c 60 -d 15s -t 4 -s load.lua http://localhost:8080`.

Получается следующий результат (сильно зависящий от того, где именно вы тестируете):
```
Running 15s test @ http://localhost:8080
  4 threads and 60 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    84.54ms  146.38ms   1.95s    96.88%
    Req/Sec   234.64     45.47   435.00     80.23%
  14027 requests in 15.10s, 1.71MB read
  Socket errors: connect 0, read 0, write 0, timeout 3
Requests/sec:    929.02
Transfer/sec:    116.13KB
```

При этом, заглянув во время теста в `htop`, мы увидим, что наш сервер грузит
CPU на 100%. Попробуем в первом приближении решить данную проблему.

### Свой балансировщик на Tarantool

#### Iproto

В лекции про протоколы мы рассматривали различные способы
взаимодействия между клиентом и сервером,
в частности на примере протокола HTTP.
В Tarantool есть свой протокол общения инстансов
друг с другом - [iproto](https://www.tarantool.io/en/doc/latest/dev_guide/internals/box_protocol/).

Iproto работает поверх TCP. И в отличие от HTTP v1.1 он бинарный и асинхронный.
При этом передаем не просто какие-то сырые бинарные данные,
а в специальном формате MessagePack.

![image](https://user-images.githubusercontent.com/8830475/111397695-f6fd9480-86d2-11eb-97d3-d75eb179022b.png)

На практике, вызов `box.cfg({listen = uri})` поднимает
iproto-сервер. Далее мы можем подключиться к этому серверу
с помощью встроенного модуля `net.box`.

```
-- Instance 1
box.cfg{listen = 3301}
box.schema.user.passwd('admin', 'test')

function sum(a, b)
    local res = a + b
    print(res)
    return res
end

box.schema.space.create('test')
box.space.test:create_index('pk')
box.space.test:replace({1, 2, 3})
```

```
-- Instance 2
netbox = require('net.box')

-- Устанавливаем соединение
c1 = netbox.connect('admin:test@localhost:3301')
-- или
c2 = netbox.connect('localhost:3301', {user = 'admin', password = 'test'})

-- Вызов хранимых процедур
c2:call('sum', {1, 2}, {timeout = 5})

-- Ping
c2:ping()

-- box-like доступ к данным
c2.space.test:select()
c2.space.test:update({1}, {{'+', 2, 1}})
```

#### Сам код

Чуть модифицируем сервер.
```lua
#!/usr/bin/env tarantool

local fio = require('fio')
local digest = require('digest')
local port = tonumber(arg[1])
if port == nil then
    error('Invalid port')
end

local work_dir = fio.pathjoin('data', port)
fio.mktree(work_dir)
box.cfg({
    listen = port,
    work_dir = work_dir,
})
box.schema.user.passwd('admin', 'test')

function exec()
    local str = string.format("I'm %s", port)
    for _ = 1, 1e2 do
        digest.sha512_hex(str)
    end
    return str
end
```

Задача балансировщика — отправка запрос на один из трех серверов.
С помощью алгоритма round robin.
```lua
#!/usr/bin/env tarantool

local log = require('log')
local netbox = require('net.box')
local http_server = require('http.server')

local hosts = {
    'admin:test@localhost:3301',
    'admin:test@localhost:3302',
    'admin:test@localhost:3303',
}

local connections = {}
for _, host in ipairs(hosts) do
    local conn = netbox.connect(host)
    assert(conn)
    log.info('Connected to %s', host)
    table.insert(connections, conn)
end

local req_num = 1
local function handler()
    local conn = connections[req_num]
    if req_num == #connections then
        req_num = 1
    else
        req_num = req_num + 1
    end

    local result = conn:call('exec')

    return {
        body = result,
        status = 200,
    }
end

local httpd = http_server.new('0.0.0.0', '8080', {log_requests = false})
httpd:route({method = 'GET', path = '/'}, handler)
httpd:start()
```

Какие проблемы у данного балансировщика?
Как их можно решить?

Тем не менее результат лучше, чем в случае с одним инстансом:
```
➜  wrk -c 60 -d 15s -t 4 -s load.lua http://localhost:8080
Running 15s test @ http://localhost:8080
  4 threads and 60 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    24.21ms   13.07ms  89.46ms   66.54%
    Req/Sec   631.96     82.51   830.00     76.50%
  37908 requests in 15.08s, 4.63MB read
Requests/sec:   2514.24
Transfer/sec:    314.28KB
```

### HAProxy

![image](https://user-images.githubusercontent.com/8830475/111398778-244b4200-86d5-11eb-9462-fb0600ed2a08.png)

```bash
apt install haproxy
yum install haproxy
brew install haproxy
git clone https://github.com/haproxy/haproxy
```

Файл конфигурации - `/etc/haproxy/haproxy.cfg`.

Тестовая конфигурация:
```
global
   log /dev/log local0
   log /dev/log local1 notice
   chroot /var/lib/haproxy
   stats timeout 30s
   user <USER>
   group <GROUP>
   daemon

defaults
   log global
   mode http
   option httplog
   option dontlognull
   timeout connect 5000
   timeout client 50000
   timeout server 50000

frontend http_front
   bind *:8080
   stats uri /haproxy?stats
   default_backend http_back

backend http_back
   balance roundrobin
   server server_name1 localhost:8081 check
   server server_name2 localhost:8082 check
   server server_name3 localhost:8083 check
```

Тестовые сервера будут всё так же на Tarantool.
```lua
#!/usr/bin/env tarantool

local port = tonumber(arg[1])
if port == nil then
    error('Invalid port')
end

local http_server = require('http.server')

local function handler()
    return {
        body = string.format("I'm %s", port),
        status = 200,
    }
end

local httpd = http_server.new('0.0.0.0', port, {log_requests = false})
httpd:route({path = '/'}, handler)
httpd:start()
```

И поднимем наши http-сервера на портах 8081-8083.
Спама в логах бояться не стоит — проблема известная: https://github.com/tarantool/http/issues/71.
Запустим haproxy при помощи `haproxy -f haproxy.cfg -V -d` - в режиме дебага.
Теперь совершая запросы на порт 8080 можно видеть, что наши запросы балансируются
с помощью алгоритма round robin между нашими серверами.
Статистика по запросам будет доступна на `http://localhost:8080/haproxy?stats`.

#### Конфигурация
Настройка HAProxy обычно сосредоточена вокруг пяти разделов: global, defaults, frontend, backend, реже listen.

Раздел **global** определяет общую конфигурацию для всего HAProxy

**Defaults** определяет настройки по-умолчанию для остальных разделов проксирования.

Раздел **listen** объединяет в себе описание для фронтенда и бэкенда и содержит полный список прокси. Он полезен для TCP трафика.

Раздел **Frontend** определяет, каким образом перенаправлять запросы к бэкенду в зависимости от того, что за запрос поступил от клиента.

Секция **Backend** содержит список серверов и отвечает за балансировку нагрузки между ними в зависимости от выбранного алгоритма

##### Frontend
* В нашем примере у секции frontend выбран **mode** __http__ - мы работаем на прикладном уровне.
Кроме этого, доступен и __tcp__ mode для работы с транспортным уровнем.

* bind. IP Адрес и порт, которые HAProxy должен слушать «на входе».

* default_backend. Название конфигурации группы серверов, к которым надо направить запрос для обработки. В данном случае запросы пойдут к backend с названием http_back.

##### Backend
В данной секции указан алгоритм балансировки - RoundRobin.
А также список серверов, между которыми мы будем балансировать запросы.

Список возможностей не ограничивается только лишь балансировкой.
Типичными кейсами являются также и редирект пользователей с HTTP на HTTPS.
Или с одного адреса на другой.
Можно добавить небольшой фрагмент в конфиг и убедиться,
что теперь при запросе на :8079 мы будем получать 301 и редирект на 8080.
```
frontend http_front_redirect
  bind *:8079
  mode http
  http-request redirect code 301 prefix localhost:8080
```

Более подробно о настройке HAProxy можно почитать в документации -
https://www.haproxy.org/download/2.4/doc/configuration.txt.

Кроме того, функциональность HAProxy под ваши нужды можно расширить
при помощт скриптов на языке Lua - https://www.haproxy.com/blog/5-ways-to-extend-haproxy-with-lua/.
(Для этого, правда, скорее всего придется собрать HAProxy из исходного кода
со специальным флагом).


### Nginx

![image](https://user-images.githubusercontent.com/8830475/111398705-0251bf80-86d5-11eb-8fda-6731e61c2ec6.png)

Этот веб-сервер считается одним из самых популярных и производительных решений,
ведь имеет широчайший функционал и гибкость при настройке.
В том числе Nginx часто используется для балансировки нагрузки.

```bash
apt install nginx
yum install nginx
brew install nginx
git clone https://github.com/nginx/nginx
```

Конфигурация лежит в `/usr/local/etc/nginx/nginx.conf`.
Проверить корректность конфигурации `nginx -t` и перезапустить сервер `nginx -s reload`.

```
events {
    worker_connections  1024;
}

http {
    upstream myapp1 {
        server localhost:8081;
        server localhost:8082;
        server localhost:8083;
    }

    server {
        listen 8080;

        location / {
            proxy_pass http://myapp1;
        }
    }
}
```

[Доступные опции](https://nginx.org/en/docs/http/load_balancing.html):


* **least_conn** - задаёт для группы метод балансировки нагрузки, при котором запрос передаётся серверу с наименьшим числом активных соединений, с учётом весов серверов. Если подходит сразу несколько серверов, они выбираются циклически (в режиме round-robin) с учётом их весов.
```
   upstream myapp1 {
        least_conn;
        server localhost:8081;
        server localhost:8082;
        server localhost:8083;
    }
```

* **weight** - задаёт вес сервера, по умолчанию 1.

```
    upstream myapp1 {
        server localhost:8081 weight=3;
        server localhost:8082;
        server localhost:8083;
    }
```

В качестве тестовых серверов будем использовать HTTP-сервера на Tarantool,
код которых приведен в предыдущем примере.


### Домашнее задание

Написать балансировщик на Tarantool:
  - Запросы принимаются по протоколу HTTP - GET на `/`;
  - Каждый запрос должен возвращать ответ в формате `<host:port>: <количество запросов за последнюю секунду на данный сервер>`;
  - Предусмотреть ситуацию, когда один из серверов может упасть — система должна продолжить распределять запросы между оставшимися серверами;
  - (*) Ограничить количество одновременных запросов, приходящих на балансировщик (по умолчанию 1000, но сделать конфигурируемым).
Возвращать status code 429 при превышении лимита;
  - Предусмотреть возможность динамически (через консоль) добавлять и удалять сервера.


(*) - необязательная часть задания для получения доп. баллов.

### Ссылки

    [1] https://xakep.ru/2014/03/15/62207/
    [2] https://selectel.ru/blog/balansirovka-nagruzki-osnovnye-algoritmy-i-metody/
