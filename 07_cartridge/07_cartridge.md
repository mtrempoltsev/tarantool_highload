## Cartridge

Мы познакомились с тем, как масштабировать Tarantool.
С тем, как работает репликация и шардинг.
Однако, по-прежнему, это не решает всех проблем и не дает всех средств для
построения надежной отказоустойчивой системы.

Одна из фундаментальных проблем - распространение и поддержание
единой конфигурации на всех инстансах кластера.
Кроме этого, необходимы были средства для удобного управления конфигурацией,
обеспечения отказоустойчивости.
При этом только разработкой дело не должно было ограничиваться -
необходим набор инструментов, которые бы позволяли доставлять
и разворачивать приложения. Это будет подробно рассмотрено в
следующий раз. Пока что из важного - для того, чтобы это было возможно,
необходимо поддерживать некоторую общую структуру проекта.

Tarantool является платформой для написания сервисов.
Обычно в проектах не работают с единичными инстансами.
Главным образом из-за однопоточности - довольно скоро инстанс начинает
загружать CPU на 100% - единственным решением проблемы является масштабирование.
При этом для локального тестирования и при разработке имеет смысл разворачивать
все сервисы на единственном инстансе.

Так появился [Cartridge](https://github.com/tarantool/cartridge) - фреймворк для написания приложений на Tarantool,
который вводит понятие ролей. Роль, по сути, является Lua-модулем.
По контракту роль должна предоставить набор функций - для инициализации/деинициализации,
валидации конфигурации и её применения.

Основные возможности Tarantool Cartridge:
* автоматизированное оркестрирование кластера;
* расширение функциональности приложения с помощью новых ролей;
* шаблон приложения для разработки и развертывания;
* встроенное автоматическое шардирование;
* управление кластером с помощью WebUI и API;
* инструменты упаковки, тестирования и деплоя;
* failover & switchover;

![image](https://user-images.githubusercontent.com/8830475/110234778-7ce44780-7f3d-11eb-94d1-5006ce00c247.png)

### Пререквизиты

Нам понадобится приложение [cartridge-cli](https://github.com/tarantool/cartridge-cli).
Не только на этом занятии, но и на следующих, а также при выполнении домашних заданий.

*Cartridge-cli* - приложение, которое упрощает создание, сборку, упаковку проектов и
управление ими.

Запуск приложения `cartridge [command]`. Но не стоит ассоциировать `cartridge`
как фреймворк для разработки приложений и `command-line interface` для него.
Это два отдельных приложения со своим релизным циклом.
При этом если cartridge большей частью написан на Lua,
то cartridge-cli на Go.

### Первое приложение

Запустим команду `cartridge create`, чтобы предсоздать небольшой шаблон для проекта.
Соберем проект - `cartridge build`. Сразу запустим проект `tarantool init.lua`.
При этом мы запустили один инстанс, теперь он запущен на `localhost:8081`.

Рассмотрим структуру проекта детальнее (актуально для версии 2.8.0).

```
.
├── Dockerfile.build.cartridge
├── Dockerfile.cartridge
├── README.md
├── app
│   ├── admin.lua
│   └── roles
│       └── custom.lua
├── cartridge.post-build
├── cartridge.pre-build
├── deps.sh
├── init.lua
├── instances.yml
├── myapp-scm-1.rockspec
├── replicasets.yml
├── stateboard.init.lua
├── test
│   ├── helper.lua
│   ├── integration
│   │   └── api_test.lua
│   └── unit
│       └── sample_test.lua
└── tmp

6 directories, 16 files
```

* Файлы "Dockerfile.*" нужны для сборки и запуска нашего
проекта с помощью docker. Подробнее на следующих занятиях;
* app - наше приложение;
* cartridge.* - специальные хуки, которые вызываются при сборке;
* deps.sh - скрипт для установки dev-зависимостей. Например, модулей тестирования;
* init.lua - входная точка для запуска нашего проекта;
* instanses.yml/replicasets.yml - файлы для запуска и конфигурирования инстансов с проектов.
instances.yml отвечает за запуск, а replicasets.yml - за сборку этих инстансов в репликасеты, назначение ролей и т.д;
* myapp-scm-1.rockspec - файл, описывающий проект для менеджера пакетов luarocks
  (tarantoolctl rocks). Здесь указываются зависимости нашего проекта, его имя и версия;
* stateboard.init.lua - файл для запуска stateboard - внешнего координатора для stateful failover;
* test - директория с заготовками для написания тестов с помощью фреймворка luatest;
* tmp - директория, куда будут складываться файлы, связанные с запущенными инстансами:
сокеты для подключения, xlog'и и snap'ы.

Для старта приложения позовем `cartridge start`.
Для нас запускаются несколько инстансов.
Важно понимать, что данная команда подходит лишь для
локальной разработки. В случае, если какой-то инстанс упадет
по какой-то причине, он не будет перезапущен.

Сформировать репликасеты и назначить им роли можно с помощью
команды `cartridge replicasets setup`.

### Роли

Попробуем написать собственные роли на базе примеров из vshard'a - у нас будут
2 роли: storage и router. Никаких особых настроек от нас не потребуется,
мы просто добавим в зависимости встроенные роли `vshard-router` и `vshard-storage`.

И сразу стоит обратить внимание на то, какая у ролей структура:
`init`, `validate_config`, `apply_config`, `stop`.
При этом все настройки картриджа производятся с помощью `cartridge.cfg()` - это некоторая
обертка над box.cfg(). Соответственно в дальнейшем использовать `box.cfg{}` не разрешается.

После таких несложных шагов у нас есть почти готовое простое приложение из
стораджей и роутера. При этом мы можем достаточно просто изменять
топологию нашего кластера - добавлять или отключать роли.

В шаблоне приложения используются встроенные роли - vshard-storage и vshard-router,
а также заготовка для нашей кастомной роли.
В лекции про vshard говорилось о важности того,
чтобы на каждом из инстансов была идентичная конфигурация.
Обеспечением этого занимается картридж.

### Vshard-группы

При этом иногда возникает необходимость хранить данные не в одном сторадже, а в нескольких.
Т.е. есть не одно хранилище данных, а несколько. При этом хочется иметь возможность обращаться
к каждому хранилищу из своего приложения. Это тоже реализуется достаточно несложно и называется
vshard-группы.
Допишем просто в `cartridge.cfg{ vshard_groups = {'default', 'new'} }` - default существует по умолчанию,
все запущенные до этого стораджа находятся в этой группе.
А в группу `new` мы добавим 2 новых инстанса - мастера и реплику.

Получить доступ к различным инстансам роутера можно с помощью
`cartridge.service_get('vshard-router').get('new')`.

#### Remote procedure call
Кроме указанных выше обязательных функций роль может определять
свои собственные публичные функции. Т.е. роль может предоставить некоторый набор функций -
интерфейс, с помощью которого другие роли могли бы взаимодействовать с ней.
Объявляются эти функции в том же месте, что и системные.
При этом одна роль может обратиться к другой с помощью функции `cartridge.rpc_call(role_name, fn_name, {args}, {opts})`.

Внутри, конечно, используется тот же netbox - стандартный способ общения Tarantool'ов друг с другом.
Однако есть и несколько опций, влияющий на то, на каком конкретно инстансе данная функция будет вызвана.
Например, `perfer_local=true` не будет делать сетевой запрос в случае, если в рамках
нашего инстанса доступна роль, функцию которой мы хотим вызвать. `perfer_leader=true` гарантирует
нам, что вызов будет сделан к инстансу-лидеру репликасета с включенной данной ролью -
это необходимо, если мы делаем пишущий запрос.

### Конфигурация

Вся конфигурация картриджа хранится в yaml-файлах. Т.е. в случае неполадок,
если по какой-то причине доступ к инстансу потерян, нам не нужен доступ к спейсам
и инстансу Tarantool как таковому - достаточно обновить файл, лежащий на
файловой системе.

Часть конфигурации является публичной - в первую очередь это секции,
которые задает сам пользователь и, например, схема данных.
Часть секций приватная - топология, пользователи...
К этой части конфигурации пользователи имеют доступ через Lua API,
GraphQL API и UI, но скачивать или обновлять напрямую (через `config_patch_clusterwide`)
эти секции не получится.

В общем случае для модификации конфигурации есть несколько способов.
* Загрузка `yml`-файла через UI;
* Редактирование и загрузка через вкладку "Code";
* Из кода приложения загрузка через Lua API или обновление с помощью `cartridge.config_patch_clusterwide`.

Далее картридж сам рассылает конфигурацию на каждый инстанс, валидирует и применяет.
Это делается с помощью двухфазного коммита.
Поэтому если возникает ошибка на этапе validate_config, то конфигурация не применяется совсем.
Ошибки на этапе init или apply_config приводят к переходу инстанса в состояние,
в котором доступны лишь повторное применения конфигурации, либо требуется ручное вмешательство.

Стоит отметить тот факт, что до применения конфигурации пользователь в состоянии
модифицировать конфиг (например, установить значения по умолчанию для каких-либо секций).
Делается это с помощью модуля `cartridge.twophase` и его функции `on_patch`,
принимающей пользовательский обработчик.

Отдельным файлом конфигурации является `.tarantool.cookie`. В нем хранится cluster-cookie -
некоторый секрет, с помощью которого инстансы авторизуют друг друга.
По факту, данная строка является паролем к пользователю `admin` - именно под этим
пользователем инстансы общаются друг с другом.

#### GraphQL API

До этого мы рассматривали только REST API - получение, обновление, удаление, добавление
с помощью GET, PUT, DELETE, POST.
При этом у нас нет возможности выбрать какое-то определенное подмножество полей.
Кроме этого, обычно у нас на одну сущность отдельный эндпоинт.
Эту проблему решает язык запросов GraphQL.

![image](https://user-images.githubusercontent.com/8830475/112021672-9642e180-8b42-11eb-82fb-034c91b2b06b.png)

Подробно узнать о данном языке запросов можно в [официальной документации](https://graphql.org/),
нас же будут интересовать конкретные примеры запросов.

Их бывает несколько видов, но мы будем рассматривать `query` и `mutation` -
запросы "читающие" и "пишущие".

```graphql
query OperationName {
    servers {
        uri
    }
}
```

```graphql
mutation turnAuth($enabled: Boolean) {
    cluster {
        authParams: auth_params(enabled: $enabled) {
             enabled
        }
    }
}
```

GraphQL запрос, сделанный с помощью утилиты curl:
```bash
curl --location --request POST 'localhost:8080/admin/api' \
--header 'Content-Type: application/json' \
--data-raw '{"query":"mutation turnAuth($enabled: Boolean) { cluster { authParams: auth_params(enabled: $enabled) { enabled } } }","variables":{"enabled":true}}'
```

### Failover

![image](https://user-images.githubusercontent.com/8830475/110230737-f5d6a580-7f23-11eb-932a-cfb2a7148c5e.png)

Никто не застрахован от проблем с сетью, проблем с питанием или ошибок в ПО.
При этом подобные сбои не должны оказывать видимого влияния на нашу систему.
Для того, чтобы устранять подобные проблемы, нам необходим механизм
аварийного переключения - `failover`.
По умолчанию, если инстанс перестает работать, никаких дествий не производится.
Продемонстрируем остановив какой-либо инстанс с помощью команды `kill -18 <pid>`.
(`pid` можно посмотреть в файлах `tmp/run/<project>.<instance>.pid`).
Если мы остановим мастера стораджа, то спустя какое-то время он передет в состояние `dead`.
При этом записывать что-либо на данный репликасет уже не получится.
При этом у нас есть реплика. И в целом, если мастера в репликасете больше нет,
то мы можем делать пишущие запросы на реплику, предварительно переключив её в `read_only=false`.

Этим и должен заниматься `failover`. Вернем инстанс обратно - `kill -19 <pid>`.
И откроем вкладку `failover`.

![image](https://user-images.githubusercontent.com/8830475/110309507-7b388380-8012-11eb-92ea-82bf3b4b974d.png)

Нам доступно несколько вариантов. Начнем с `eventual` режима. Этот режим появился исторически раньше,
не является надежным, но в целом подходит, для локальной разработки, например, т.к. не требует
никаких зависимостей. Теперь при остановке сервера мы увидим, что лидер в репликасете поменялся,
т.е. пишущие запросы не будут падать. Восстановив работоспособность прежнего мастера, мы увидим,
что новый мастер перестанет быть таковым и вернет "корону" прежнему мастеру.

Почему это плохо?
Работоспособность данного `failover` основывается на алгоритме SWIM, который
не гарантирует строгой консистентности в какой-то момент времени.
Поэтому не исключена ситуация, при которой в репликасете будут 2 лидера -
что может привести к потере данных и конфликтам репликации.

Более надежным является `stateful failover` в отличие от `eventual` он дает
больше гарантий консистентности. Это происходит, главным образом, за счет того, что
лидер определяется не с помощью слухов в кластере, а при помощи внешнего координатора.
Таким образом в кластере будет всегда один лидер.
Но это требует того самого внешнего координатора - пока доступны всего 2.
Это отдельный инстанс Tarantool - `stateboard` или `etcd2`.
Также требуется назначить роль `failover-coordinator` - данная роль взаимодействует со
`state provider` и принимает все решения. В случае, если запущено несколько таких ролей -
принимать решения сможет тот, кто самый первый захватит блокировку.

Теперь проделаем тоже, что и в предыдущем примере - отключим инстанс, убедимся, что мастер
действительно переключается, восстановим инстанс, и увидим, что старый мастер стал простой
репликой. И это не просто так. За время, пока он не был доступен, могло произойти достаточно большое
количество изменений, поэтому нельзя просто взять и переключить мастера обратно -
это может привести к потере данных.
Для переключения мастера есть специальный механизм - `switchover`.
Который бывает консистентным и нет. Консистентность значит, что в случае асинхронной репликации
мы не переключим мастера, пока данные не будут синхронизированы.
Без консистентности у нас нет таких гарантий, а значит и часть данных может быть потеряна.

Stateful failover не дает защиты от проблемы split-brain - разделения кластера
на независимые части. Для решения этой проблемы реализован fencing - функциональность,
которая следит за тем, чтобы был собран кворум. При потере кворума инстансы автоматически
переключаются в read-only режим.

Наконец, стоит вспомнить алгоритм `RAFT`, который обсуждался до этого.
Да, с помощью него можно всё это организовать.
При этом данная функциональность достаточно новая - в ней всё ещё есть недоделки,
она не будет работать со старыми версиями, добавить в `Cartridge` её просто не успели.
