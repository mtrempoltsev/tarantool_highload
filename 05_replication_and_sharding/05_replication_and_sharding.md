### Репликация

Репликация — это процесс создания копий данных из одного хранилища в другом.
Каждая копия называется репликой. Репликация может использоваться, если нужно получить резервную копию,
реализовать hot standby (выполнять запросы на чтение на репликах)
или горизонтально масштабировать систему.
А для этого необходимо иметь возможность использовать одни и те же данные на разных узлах вычислительной сети кластера.

Классификация репликации:

* Направление: **master-master** или **master-slave**.
Master-slave репликация — это самый простой вариант.
У вас есть один узел, на котором вы меняете данные.
Эти изменения вы транслируете на остальные узлы, где они применяются.
При master-master репликации изменения вносятся сразу на нескольких узлах.
В этом случае каждый узел и сам изменяет свои данные, и применяет к себе изменения, сделанные на других узлах.

* Режим работы: **асинхронная** или **синхронная**. Синхронная репликация подразумевает,
что данные не будут зафиксированы и пользователю не будет подтверждено выполнение репликации до тех пор,
пока изменения не распространятся хотя бы по минимальному заданному количеству узлов кластера.
В асинхронной репликации фиксация транзакции (её коммит) и взаимодействие с пользователем —
это два независимых процесса.
Для коммита данных требуется только, чтобы они попали в локальный журнал,
и уже потом эти изменения каким-либо образом транслируются на другие узлы.
Очевидно, что из-за этого у асинхронной репликации есть ряд побочных эффектов.

### Немного терминологии

* LSN (Log sequence number) - порядковый номер операции на сервере.
  Каждый сервер при выполнении операции каждой полученной строке лога присваивает увеличивающийся номер.

* Vclock - это вектор последних lsn, применённых относительно каждого узла кластера.

![image](https://user-images.githubusercontent.com/8830475/113600920-5c490380-9649-11eb-9454-f76413ad3277.png)

Работа с vclock в общем случае происходит по следующим правилам:
  - Локальное событие приводит к увеличению компоненты vclock'a.
  - При получении сообщения с vclock'ом вычисляем новый vclock  - расчет
    покомпонентного максимума локального и "удаленного" vclock'ов.

### Репликация в Tarantool

![image](https://user-images.githubusercontent.com/8830475/109418286-b3b7dc00-79d8-11eb-9297-d67f2fb34642.png)

* Она строится из базовых кирпичиков, с помощью которых вы можете создать кластер любой топологии.
Каждый такой базовый элемент конфигурации является однонаправленным,
то есть у вас всегда есть master и slave.
Master выполняет какие-то действия и формирует лог операций, который применяется на реплике.
Репликация мастер-мастер в этом случае - просто создание ещё одного направления репликации в обратную сторону.

![image](https://user-images.githubusercontent.com/8830475/109418238-5c197080-79d8-11eb-9d77-23cd8bab4c7d.png)

* По умолчанию репликация в Tarantool асинхронная, однако есть возможность создания
синхронных спейсов. Все транзакции, которые оперируют с такими спейсами будут синхронными.
В асинхронном случае система подтверждает вам коммит независимо от того,
сколько реплик эту транзакцию увидели, сколько её к себе применили и получилось ли вообще это сделать.
Синхронный вариант требует, чтобы транзакцию подтвердили как минимум несколько узлов
(данный параметр конфигурируем).
Также синхронная репликация работает только в случае master-slave.

* Ещё одно свойство репликации в Tarantool — она построчная (row-based).
Tarantool ведёт внутри себя журнал операций (WAL). Операция попадает туда построчно,
то есть при изменении какого-то тапла из спейса эта операция записывается в журнал как одна строка.
Далее в случае асинхронной репликации операция сразу подтверждается,
после этого фоновый процесс считывает эту строку из журнала и отправляет её реплике.
Сколько у master‘а реплик, столько у него фоновых процессов (relay).
То есть каждый процесс репликации на разные узлы кластера выполняется асинхронно от других.
В случае синхронной репликации запись попадает специальную очередь — limbo,
где дожидаются своей репликации на кворум узлов.
Когда кворум подтверждений собран, транзакция коммитится, иначе откатывается.
Только после этого управление возвращается пользователю.

* Каждый узел кластера имеет свой уникальный идентификатор (instance_uuid),
который генерируется при создании узла.
Кроме того, узел имеет также идентификатор в кластере.
Это численная константа, которая присваивается реплике при подключении к кластеру,
и она остаётся вместе с репликой в течение всего времени её существования в кластере.

### Настройка репликации
Попробуем настроить репликацию master-slave.
```lua
-- master.lua
box.cfg({
  listen = 3301,
  replication = {
    'replicator:password@127.0.0.1:3301',  -- URI мастера
    'replicator:password@127.0.0.1:3302',  -- URI реплики
  },
  read_only = false,
})

box.schema.user.create('replicator', {password = 'password', if_not_exists = true})
box.schema.user.grant('replicator', 'replication', nil, nil, {if_not_exists = true})
```

```lua
-- slave.lua
box.cfg({
  listen = 3302,
  replication = {
    'replicator:password@127.0.0.1:3301',  -- URI мастера
    'replicator:password@127.0.0.1:3302',  -- URI реплики
  },
  read_only = true,
})
```

Запустим оба скрипта с помощью `tarantool -i file`.
Файлы необходимо запускать в разных директориях.
Или конфигурировать отдельно, где они будут хранить свои WAL (параметр `work_dir`).

Попытка выполнить (почти) любую пишущую операцию на реплике приведет к ошибке
`Can't modify data because this instance is in read-only mode`.
Пишущие операции над некоторыми типами спейсов (temporary и local) доступны и на репликах.

По умолчанию, пока все реплики не подключены, управлению пользователю не вернется.
Это можно конфигурировать с помощью опции `replication_connect_quorum`.
При выставлении этой опции в 0 мы не будем дожидаться подключения всех реплик.

Состояние репликации можно мониторить через `box.info.replication`:
```yaml
---
- 1:
    id: 1
    uuid: e697205f-b7d2-4a19-a8b5-3d738649744d
    lsn: 6
  2:
    id: 2
    uuid: 5f4ad913-fb8f-476c-9278-f3d8c505d2e4
    lsn: 0
    upstream:
      status: follow
      idle: 0.49817899998743
      peer: replicator@127.0.0.1:3302
      lag: 0.0001680850982666
    downstream:
      status: follow
      idle: 0.13905499997782
      vclock: {1: 6}
...
```

Попробуем добавить ещё один инстанс в наш репликасет.
Для этого сделаем небольшое исправление в скрипте `slave.lua` - поменяем порт, который этот инстанс будет слушать.
```yaml
---
- 1:
    id: 1
    uuid: e697205f-b7d2-4a19-a8b5-3d738649744d
    lsn: 7
  2:
    id: 2
    uuid: 5f4ad913-fb8f-476c-9278-f3d8c505d2e4
    lsn: 0
    upstream:
      status: follow
      idle: 0.78189400001429
      peer: replicator@127.0.0.1:3302
      lag: 8.702278137207e-05
    downstream:
      status: follow
      idle: 0.53659000000334
      vclock: {1: 7}
  3:
    id: 3
    uuid: fd09b908-5162-42fe-96aa-7d8736165673
    lsn: 0
    downstream:
      status: follow
      idle: 0.50591900001746
      vclock: {1: 7}
...
```

Вот так достаточно просто получилось добавить новую реплику.
Попробуем создать спейс и добавить в него запись (на мастере).
Теперь если мы попытаемся посмотреть содержимое спейса, то мы увидим,
что данные попали на реплики.
Напомню, что репликация асинхронная.
Мы получили ответ раньше, чем данные были сохранены на репликах.

Отключим наши реплики.
```yaml
---
- 1:
    id: 1
    uuid: e697205f-b7d2-4a19-a8b5-3d738649744d
    lsn: 11
  2:
    id: 2
    uuid: 5f4ad913-fb8f-476c-9278-f3d8c505d2e4
    lsn: 0
    upstream:
      peer: replicator@127.0.0.1:3302
      lag: 0.00014305114746094
      status: disconnected
      idle: 11.628826999979
      message: connect, called on fd 18, aka 127.0.0.1:60770
      system_message: Connection refused
    downstream:
      status: stopped
      message: 'unexpected EOF when reading from socket, called on fd 18, aka [::ffff:127.0.0.1]:3301,
        peer of [::ffff:127.0.0.1]:'
      system_message: Broken pipe
  3:
    id: 3
    uuid: fd09b908-5162-42fe-96aa-7d8736165673
    lsn: 0
    downstream:
      status: stopped
      message: 'unexpected EOF when reading from socket, called on fd 23, aka [::ffff:127.0.0.1]:3301,
        peer of [::ffff:127.0.0.1]:'
      system_message: Broken pipe
...
```
Попытка записать что-то новое в спейс не вызовет ошибку.
При этом если мы поднимем наши реплики заново, то увидим, что через какое-то
время данные на мастере и репликах будут синхронизированы.

Теперь попробуем создать синхронный спейс.
И выставим `box.cfg{replication_synchro_quorum = 3, replication_synchro_timeout = 5}`.
Количество реплик, которые должно подтвердить транзакцию - 3 за 5 секунд.
Теперь если мы попробуем выключить реплики и сделать какие-то изменения, мы получим следующую ошибку:
`Quorum collection for a synchronous transaction is timed out` - за 5 секунд мы не получили подтверждение от
трех реплик (сам инстанс считается за реплику), поэтому транзакция откатилась.

Подробнее остановимся на том, что мы видим в `box.info.replication`:
* Сущность `upstream`. Атрибут `status follow` означает, что инстанс следует за master'ом.
`Idle` — время, которое прошло локально с момента последнего взаимодействия с этим master'ом.
Мы не шлём поток непрерывно, master отправляет дельту, только когда на нём происходят изменения.
Когда мы отправляем какой-то ACK, мы тоже осуществляем взаимодействие.
Если idle становится большим (секунды, минуты, часы), то что-то не так.
* Атрибут `lag`. Кроме `lsn` и `server id` каждая операция в логе маркируется еще и timestamp’ом — локальным временем,
  в течение которого данная операция была записана в vclock на master'е, который её выполнил.
  Slave при этом сравнивает свой локальный timestamp с timestamp’ом дельты, которую он получил.
  Последний текущий timestamp, полученный для последней строчки, slave выводит в мониторинге.
* Атрибут `downstream`. Он показывает то, что master знает о своём конкретном slave'е.
Это ACK, который slave ему отправляет.

![image](https://user-images.githubusercontent.com/8830475/110230491-667cc280-7f22-11eb-99e6-32926dd30645.png)

С точки зрения же системы на каждое соединение у нас порождается поток (thread).
Для источника этот поток называется relay, для приемника - applier.

#### Конфликты репликации

Попробуем сделать следующие шаги:

- Отключить репликацию между инстансами;
- На каждом из инстансов сделаем insert тапла с одинаковым ключом;
- Вернем репликацию обратно.

Ожидается, что мы увидим следующее сообщение в логах:
```log
2021-04-01 21:48:27.760 [3242] main/130/applier/replicator@127.0.0.1:3301 applier.cc:289 E> error applying row: {type: 'INSERT', replica_id: 1, lsn: 14, space_id: 512, index_id: 0, tuple: [4, 4, 4]}
2021-04-01 21:48:27.760 [3242] main/130/applier/replicator@127.0.0.1:3301 I> can't read row
2021-04-01 21:48:27.760 [3242] main/130/applier/replicator@127.0.0.1:3301 memtx_tree.cc:779 E> ER_TUPLE_FOUND: Duplicate key exists in unique index 'pk' in space 'test'
2021-04-01 21:48:27.760 [3242] main/103/init.lua C> failed to synchronize with 1 out of 2 replicas
```

И в `box.info.replication`
```yaml
tarantool> box.info.replication
---
- 1:
    id: 1
    uuid: 8e64c535-9368-4b8e-83a1-5496389b495a
    lsn: 13
    upstream:
      peer: replicator@127.0.0.1:3301
      lag: 28.781993865967
      status: stopped
      idle: 18.933447999996
      message: Duplicate key exists in unique index 'pk' in space 'test'
    downstream:
      status: stopped
      message: 'unexpected EOF when reading from socket, called on fd 16, aka [::ffff:127.0.0.1]:3302,
        peer of [::ffff:127.0.0.1]:'
      system_message: Broken pipe
  2:
    id: 2
    uuid: 3285c1c2-11d8-4af9-98aa-007efcfa3fbf
    lsn: 1
...
```

Так и выглядят конфликты репликации.
И это дает небольшое представление о том, как именно репликация работает.
Тарантул реплицирует отдельные операции над таплами - insert, replace, update.
И в данном случае после того, как был сделан один апдейт на инстансе,
на него реплицируется другой - это приводит к ошибке - insert возможен лишь если такого
первичного ключа не существует.

Как решать эту проблему?

- Использовать операции, которые не приводят к конфликтам - replace вместо insert;
- Использовать before_replace триггер для решения конфликта.

### Спейсы без репликации

При создании можно указать флаг `is_local`, тогда содержимое
спейса не будет реплицироваться. Также доступен флаг `temporary` -
данные сохраняются в WAL, поэтому содержимое таких спейсов также не реплицируется.

То, что содержимое спейсов не реплицируется не значит, что не реплицируется
факт создания таких спейсов (DDL). Одновременное создание таких спейсов
на нескольких узлах также может привести к конфликтам репликации
(информация о спейсах хранится в системном спейсе `_space`).

### Шардинг

![image](https://user-images.githubusercontent.com/8830475/113601654-4851d180-964a-11eb-9d73-90151d2ee781.png)

Если репликация служит для масштабирования вычислений, то шардинг — для масштабирования данных.
(Имея несколько вычислительных узлов, мы можем параллельно проводить на них вычисления,
но что если наши данные перестают помещаться на одном сервере).

Шардинг делится еще на два типа: шардинг диапазонами и шардинг хешами.

При шардинге диапазонами мы от каждой записи в кластере вычисляем некоторый шард-ключ.
Эти шард-ключи проецируются на прямую линию, которая делится на диапазоны,
которые мы складываем на разные физические узлы.

![image](https://user-images.githubusercontent.com/8830475/109938534-30043500-7ce1-11eb-8f24-88db55f4c3cc.png)

Метод, используемый в Tarantool - шардирование хэшами.
Модуль, используемый для этого - [vshard](https://github.com/tarantool/vshard).
Шардинг хешами проще: от каждой записи в кластере считаем хеш-функцию,
записи с одинаковым значением хеш-функции складываем на один физический узел.

Начнем с простого примера. У нас есть какой-то предмет, например, книга,
и у книги есть некоторый числовой идентификатор. При этом книги с четным id мы складываем на первый сервер,
а книги с нечетным id на другой сервер.
Теперь, если нам нужна "четная" книга, то мы должны искать на первом сервере,
иначе на другом. Если нам нужно найти книгу по автору, то мы должны выполнять этот поиск
на обоих серверах.

Данный пример демонстрирует саму концепцию шардинга.
Однако обладает, как минимум, одной проблемой: что делать, если нам потребовалось добавить ещё один сервер?
В общем случае пересчитывать шард-функцию для всех записей и выполнять решардинг - физический перенос
данных с одного сервера на другой. Причем хотелось бы, чтобы это происходило автоматически и не требовало
никаких действий со стороны пользователя.

Концептуально vshard решает такую проблему по-другому.
Да, мы продолжаем вычилять некоторую хэш-функцию от нашего ключа шардирования (в примере выше - id),
но привязываем результат не к физическим серверам, а к некоторым виртуальным бакетам.
Количество бакетов задается самим пользователем при настройке кластера и не может меняться в будущем,
поэтому рекомендуемое значение - много больше, чем физических серверов.
Далее каждый бакет привязывается к физическому серверу.
Решардинг в таком случае не будет требоваться совсем - он заменяется другой операцией - ребалансингом.
Переносом бакетов на другой сервер.

Вернемся к нашему примеру с книгами.
Выберем количество бакетов равное 3000 и функцию шардирования - остаток от деления на 3000.
Далее при наличии 2х серверов у нас на каждом будет находиться по 1500 бакетов.
Если мы добавляем 3, 4, 5, ... сервера, нам необходимо просто переместить бакеты со старых серверов на новые.
И в случае с 3 серверами у нас будет находиться по 1000 бакетов на каждом.

VShard состоит из двух подмодулей: vshard.storage и vshard.router.
Их можно независимо создавать и масштабировать даже на одном инстансе.
При обращении к кластеру мы не знаем, где какой bucket лежит, и за нас его по bucket id будет искать vshard.router.

![image](https://user-images.githubusercontent.com/8830475/110206819-199cdb80-7e91-11eb-9d6a-831ff6501711.png)

Работа с данными сводится в итоге к следующим действиям:
  * (на сторадже) Прежде всего создать у спейса с данными индекс `bucket_id` (иначе спейс не будет учитываться vshard'ом).
  * (на роутере) Вычислить bucket_id нужного репликасета
  * Сделать запрос к данному репликасету
    * Запрос на запись будет отправлен на мастер
    * Запрос на чтение будет отправлен на одну из доступных реплик

Для демонстрации зайдем в репозиторий vshard и запустим [пример](https://github.com/tarantool/vshard/tree/master/example):

```bash
git clone https://github.com/tarantool/vshard/tree/master/vshard
tarantoolctl rocks make
cd example
make start
make enter
```

#### Конфигурация

В этой же директории в файле `localcfg.lua` можно увидеть
конфигурацию роутера и в `storage.lua` - для стораджа:
```lua
cfg = {
    listen = 3301,
    memtx_memory = 100 * 1024 * 1024,
    sharding = {
        ['cbf06940-0790-498b-948d-042b62cf3d29'] = { -- replicaset #1
            replicas = {
                ['8a274925-a26d-47fc-9e1b-af88ce939412'] = {
                    uri = 'storage:storage@127.0.0.1:3301',
                    name = 'storage_1_a',
                    master = true
                },
                ['3de2e3e1-9ebe-4d0d-abb1-26d301b84633'] = {
                    uri = 'storage:storage@127.0.0.1:3302',
                    name = 'storage_1_b'
                }
            },
        }, -- replicaset #1
        ['ac522f65-aa94-4134-9f64-51ee384f1a54'] = { -- replicaset #2
            replicas = {
                ['1e02ae8a-afc0-4e91-ba34-843a356b8ed7'] = {
                    uri = 'storage:storage@127.0.0.1:3303',
                    name = 'storage_2_a',
                    master = true
                },
                ['001688c3-66f8-4a31-8e19-036c17d489c2'] = {
                    uri = 'storage:storage@127.0.0.1:3304',
                    name = 'storage_2_b'
                }
            },
        }, -- replicaset #2
    }, -- sharding
    replication_connect_quorum = 0,
}

-- для роутера
vshard.router.cfg(cfg)

-- для стораджа
vshard.storage.cfg(cfg, instance_uuid)
```

Важно понимать, что конфигурация не является статической.
Рано или поздно что-то упадет, сломается или просто закончится место.
Потребуется либо убрать, либо добавить инстанс.
Это потребует изменения конфигурации на **всех** инстансах.
Кроме этого, желательно, чтобы это происходило синхронно.


#### Работа с данными

Мы подключимся к роутеру, на стораджах при этом будет подготовлена схема данных для работы.
Попробуем вставить какой-то объект:
```lua
customer = {
    customer_id = 1,
    bucket_id = nil,
    name = 'Ivan',
    accounts = {
        {
            account_id = 1,
            name = 'First',
        },
        {
            account_id = 2,
            name = 'Second',
        }
    },
}

customer.bucket_id = vshard.router.bucket_id_mpcrc32(customer.customer_id)

vshard.router.callrw(customer.bucket_id, 'customer_add', {customer})
```

Теперь если мы захотим прочитать данные нашего customer'a по id, нам нужно
вызвать функцию `customer_lookup` на нужном репликасете.

```
customer_id = 1
bucket_id = vshard.router.bucket_id_mpcrc32(customer_id)
vshard.router.callro(bucket_id, 'customer_lookup', {customer_id})
```

На что стоит обратить внимание?
В качестве функции шардирования используется одна из встроенных функций - `bucket_id_mpcrc32`.
При желании можно использовать любую свою функцию, главное,
чтобы она возвращала значение не превышающее количество бакетов.
При этом желательно, чтобы данные распределялись равномерно между стораджами -
если один из стораджей загружен полностью, а другой пуст, то толку от такого шардинга мало.

Кроме этого, видно, что вместе с сущностью `customer` лежат ещё и `account`s.
Причем все счета одного пользователя имеют тот же bucket id, что и этот пользователь.
Это не просто так. Подобный подход обеспечивает атомарность - у нас есть гарантия, что
все счета пользователя будут лежать на одном сторадже вместе с этим пользователем.

И в этом есть свой компромисс: с одной стороны, нам приходится вручную вычислять bucket_id,
с другой, это бывает достаточно удобно, когда есть какая-то "главная сущность" и одна или несколько
зависимых от неё - в этом случае подобный подход позволяет держать эти данные на одном сторадже.

#### Зоны и доступность на чтение

![image](https://user-images.githubusercontent.com/8830475/112019553-993cd280-8b40-11eb-9f22-31a79e9d5dbd.png)

Каждой реплике и каждому роутеру присваиваем номер зоны и задаем таблицу,
в которой указываем расстояние между каждой парой зон.
Когда роутер принимает решение, куда отправить запрос на чтение, он выберет реплику в той зоне,
которая ближе всего к его собственной.

![image](https://user-images.githubusercontent.com/8830475/112019693-bd98af00-8b40-11eb-9092-94915c2fd339.png)

В общем случае можно обращаться и к произвольной реплике, но если кластер большой и сложный,
очень сильно распределенный, тогда зонирование сильно пригодится.
Зонами могут быть разные серверные стойки, чтобы не загружать сеть трафиком.
Или это могут быть географически удаленные друг от друга точки.

Также зонирование помогает при разной производительности реплик.
К примеру, у нас в каждом replica set’е есть одна бэкап-реплика,
которая должна не принимать запросы, а только хранить копию данных.
Тогда мы делаем её в зоне, которая будет очень далеко от всех роутеров в таблице,
и они станут обращаться к ней в самом крайнем случае.


#### Failover (аварийное переключение)

Выборы нового мастера в vshard'e не реализованы, поэтому в случае необходимости их
придется делать это самостоятельно.
Когда мы его каким-то образом выбрали, нужно, чтобы этот инстанс теперь взял на себя полномочия мастера.
Обновляем конфигурацию, указав для старого мастера `master = false`, а для нового — `master = true`,
применим через VShard.storage.cfg и раскатаем на хранилища.
Дальше всё происходит автоматически.
Старый мастер перестает принимать запросы на запись и начинает синхронизацию с новым,
потому что могут быть данные, которые уже применились на старом мастере,
а на новый ещё не доехали.
После этого новый мастер вступает в роль и начинает принимать запросы,
а старый мастер становится репликой. Так работает write failover в VShard.

```
replicas = new_cfg.sharding[uud].replicas
replicas[old_master_uuid].master = false
replicas[new_master_uuid].master = true
vshard.storage.cfg(new_cfg)
```

### Домашнее задание

* Создать шардированный спейс (с помощью модуля `vshard`) `products` со следующими полями:
    * uuid - генерировать с помощью модуля `uuid` (первичный ключ);
    * category - категория продукта (молоко, сметана, помидоры) (неуникальное поле);
    * name - название (например, "Домик в деревне", "Простоквашино", ...);
    * weight - вес одной упаковки;
    * count - количество упаковок на складе.

* Реализовать следующий интерфейс для работы с данными:
    * put - сохранить информацию о товаре (если товар уже существует - ошибка);
    * update - обновить информацию о товаре (если товара не существует - ошибка);
    * get - получить информацию об товаре по uuid;
    * calculate_weight(category) - посчитать суммарный вес товаров в данной категории;
    * get_products_by_category(category, limit, after) - получить список товаров указанной категории;

#### Комментарий
Подробнее рассмотрим функцию `get_products_by_category`.

Предположим, у нас 3 стораджа с данными:

```
    Storage 1           Storage2           Storage3
    Pepsi               Baikal               7UP
    Coca-cola           Dr.Pepper           Tarrago
    Fanta               Duchess             Mountain Dew
```

Запрос `get_products_by_category('soda', 2)` должен вернуть нам
результат: `[7UP, Baikal]`,
`get_products_by_category('soda', 4)` - `[7UP, Baikal, Dr.Pepper, Duchess]`.

При этом должна быть возможность получать объект после какого-то:
`get_products_by_category('soda', 2, {Baikal, uuid_of_Baikal})` - `[Dr.Pepper, Duchess]`.

На что стоит обратить внимание:
    * Категория не является уникальным полем, однако порядок должен поддерживаться.
Единственным уникальным полем является UUID товара, внутри себя Tarantool при сортировке
по вторичному ключу учитывает ещё и первичный. Стоит сразу добавить поле UUID во вторичный ключ.
Тогда after можно будет использовать следующим образом - `space:pairs(after)`.
    * Использовать опцию `offset` в функции `select` запрещено - она работает за `O(N)`.
    * (*) Запрос `get_products_by_category` будет возвращать отсортированный массив данных
с каждого стораджа. Подумайте, как оптимальнее всего объединять несколько
отсортированных массивов.
   * Не стоит задумываться о том, как должна вести себя система в случае неполадок -
в данном случае считаем, что всё надежно и работает.
   * В качестве основы можно использовать пример прямо из репозитория.
